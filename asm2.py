# -*- coding: utf-8 -*-
"""ASM2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TCF4zoNnGuGKxh7dK0hydc9nI8uiMIce
"""

 

"""# **Import Everything**"""
# !pip install pyspark
# from google.colab import drive
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import explode_outer
from pyspark.sql.functions import col, round
from pyspark.sql.functions import udf
from pyspark.sql.functions import split
from pyspark.sql.functions import row_number
from pyspark.sql.types import *
from pyspark.sql import Window
from pyspark.sql.functions import to_json, spark_partition_id, collect_list, struct


#drive.mount('/content/drive')

"""# **Data Initialization**"""

spark = SparkSession \
    .builder \
    .appName("COMP5349 A2 Data Loading Example") \
    .getOrCreate()

## Read data and explode data, because data is an array, so use explode to splite array item into multiple rows
test_data = "s3://comp5349-2022/test.json"
test_init_df = spark.read.json(test_data).select(explode('data').alias('data'))

## explode paragraphs into multiple rows, include title as well, which specify different contracts
test_data_df= test_init_df.select(col('data.title').alias('title'),(explode("data.paragraphs").alias('paragraphs')))

test_data_df.printSchema()

"""# **Data Processing And Collect All Samples**"""

## get context value, set it into one column
context_df= test_data_df.select('title',col("paragraphs.context").alias('context'),col("paragraphs.qas").alias('qas'))
context_df.printSchema()

## UDF function to seperate context
## the start position is 0 and end position is 4096 at the beginning
## everytime the window will move right for 2048 
## when the right is reach(or outside) end, then stop right at end point
## when the left reach (or outside) end, stop loop
## return (seperate value, start point, end point)
def seperate_word(context):
    start_end = []
    start = 0
    end = 4096
 
    while start < len(context):
      info = (context[start:end],start,end)
      start_end.append(info)
   
      start += 2048
      end += 2048
      if end > len(context):
        end = len(context)
    return start_end

## use this schema to define contract UDF return type
context_schema = StructType([ \
    StructField("source",StringType(),True), \
    StructField("start",IntegerType(),True), \
    StructField("end",IntegerType(),True)
  ])

## apply contextUDF to column context, and will return a list of (context, start, end) in one row
## use explode to seperate the list into multiple rows, expand with qas and title
contextUDF = udf(seperate_word,ArrayType(context_schema))
context = context_df.select('title',contextUDF(col('context')).alias('context'),'qas') 
context_expand_df = context.select('title',explode('context').alias('context'),'qas')

## expand question
qas_df = context_expand_df.select('title','context',explode('qas').alias('qas'))

## expand answer, use explode_outer because some rows' answer value is null
qas_expand = qas_df.select('title','context.*',col('qas.id').alias('qas_id'),'qas.question','qas.is_impossible',explode_outer('qas.answers').alias('answers'))
qas_expand.printSchema()

## expand answers' start position and text

answer = qas_expand.select('title','source','start','end','qas_id','question','is_impossible','answers.*')

## UDF to define the sample is positive, impossible negative or possible negative

def negative_positive(start, end, is_impossible, answer_start, text):
  if is_impossible == True:
    return ('impossible_negative',0,0)
  else:
    answer_end = answer_start+len(text)
    if start <= answer_start and answer_end <= end:
      return ('positive',answer_start-start, answer_end-start)
    elif start<= answer_start and answer_end > end and answer_start < end:
      return ('positive', answer_start-start , end-start)
    elif answer_start < start and answer_end <= end and start < answer_end:
      return ('positive', 0 , answer_end-start)
    else:
      return ('possible_negative',0,0)

## answerSchema to define the return answer after apply udf to start, end, is_impossible, answer_start, text column

answerSchema = StructType([ \
    StructField("is_impossible",StringType(),True), \
    StructField("answer_start",IntegerType(),True), \
    StructField("answer_end",IntegerType(),True)
  ])

## apply udf to start, end, is_impossible, answer_start, text

answer_with_positionUDF = udf(negative_positive,answerSchema)
qas_impossible_answer = answer.select('title','source','start','end','qas_id','question',\
                        answer_with_positionUDF(col('start'),col('end'),col('is_impossible'),col('answer_start'),col('text')).alias('answer_state'),\
                        'text')
qas_impossible_answer.printSchema()

## Get the result of samples after all expand and change is_possible attribute

expand_df = qas_impossible_answer.select('title','source','start','end','qas_id','question','answer_state.*',col('text').alias('answer_text')).coalesce(400)
 
expand_df.count()

## show the result of all samples

expand_df.show()

"""# **Make Impossible Negative Samples Number Equals to Average**"""

## filter impossible negative samples in a contract
negative_question = expand_df.select('title','qas_id','question','is_impossible','answer_start','answer_end','source')\
                              .where(col('is_impossible')=='impossible_negative')
 
negative_question.count()

## filter out positive samples in a contract

positive = expand_df.select('title','source','question','qas_id','answer_start','answer_end','is_impossible')\
                              .where(col('is_impossible')=='positive')
 
## group positive samples by title and question, count how many samples are positive in specific contract question
positive_question = positive.groupby('title','question')\
                            .count()\
                 
positive_question.count()

positive.printSchema()

## calculate the positive sample average among contracts
## do an average operation into these counted number

positive_avg = positive_question.groupby('question')\
                  .avg('count')\
                  .select('question',round('avg(count)').alias('average')) 
                
positive_avg.count()

## Use window partitionBy to give all row number and group by question, inorder to choose rows
## if the row number (question order) is less or equal to average number of that row, then the result can be chose
## references https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html

question_window = Window.partitionBy("question").orderBy("question")

## the first join is skip the same source samples which already positive
## the second join is join positive_avg table to get the column average
negative = negative_question\
                 .join(positive, "title" and "source","leftanti")\
                 .join(positive_avg , 'question','left')\
                 .withColumn("question_order", row_number().over(question_window))\
                 .select('*')\
                 .where(col('question_order') <= col('average'))\

negative.printSchema()

## show impossible_negative result
negative.show()

## count the impossible_negative result
negative.count()

"""# **Make Possible Negative Samples Number Equal To Positive Samples Number**"""

## Filter out all possible negative samples
possible_negative_question = expand_df.select('qas_id','question','is_impossible','answer_start','answer_end','source')\
                              .where(col('is_impossible')=='possible_negative')
 
possible_negative_question.count()

## count all positive samples of different questions in different contracts                        
positive_all= positive.groupby('qas_id')\
                       .count()
positive_all.count()

## Use window partitionBy to give all row number and group by question id, inorder to choose rows
## if the row number (question order) is less or equal to total number(count) of that row, then the result can be chose
qas_window = Window.partitionBy("qas_id").orderBy("qas_id")

## the first join is join positive_all dataframe, to get the attribute count
## the second and third join is to skip the same source samples which already positive or impossible_negative

possible_negative = possible_negative_question.join(positive_all , 'qas_id','left')\
                 .join(positive,'title' and "source","leftanti")\
                 .join(negative,'title' and "source","leftanti")\
                 .withColumn("question_order", row_number().over(qas_window))\
                 .select('*')\
                 .where(col('question_order') <= col('count'))\

possible_negative.printSchema()

## show the possible negative result
possible_negative.show()

## count the number of possible negative result
possible_negative.count()

"""# **Union impossible_negative, possible_negative and possible result**"""

final_df = negative.select('source','question','answer_start','answer_end')\
.union(positive.select('source','question','answer_start','answer_end')\
       .union(possible_negative.select('source','question','answer_start','answer_end')))

## show the final result
final_df.show()

## count the result
final_df.count()

"""# **Output Result To JSON File**"""

## write file
## references https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.coalesce.html

final_list = ",\n".join(final_df.coalesce(1).toJSON().collect())

f = open('myresult.json', "w")
f.write("[ ")
f.write(final_list)
f.write(" ]")
f.close()

spark.stop()
